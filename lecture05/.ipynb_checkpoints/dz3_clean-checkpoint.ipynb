{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ou3jfUSio5rL"
   },
   "source": [
    "Горностаев Александр, ML-11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "cQ4bT6_fo60t"
   },
   "outputs": [],
   "source": [
    "# at first, mount the drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "iuzaJT_vsYCs"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "IDk4tDcFKq_9"
   },
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import make_column_selector\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from scipy.stats import rv_discrete\n",
    "\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.compose import make_column_transformer\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "5G44dyyVFbpM"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "imputes values from distribution for each given column\n",
    "target missing values are np.nan\n",
    "'''\n",
    "class DiscreteDistributionImputer():\n",
    "  def __init__(self, random_state=42):\n",
    "    # contains distributions to generate random stuff from\n",
    "    self.distributions = []\n",
    "    # to fix randomness for result recheck\n",
    "    self.random_state = random_state\n",
    "\n",
    "  def fit(self, X, y=None):\n",
    "    columns = []\n",
    "\n",
    "    if isinstance(X, pd.DataFrame):\n",
    "      columns = X.values.T\n",
    "    elif isinstance(X, np.ndarray):\n",
    "      columns = X.T\n",
    "    else:\n",
    "      print(\"given X is neither DataFrame nor ndarray\")\n",
    "\n",
    "    # clear distributions\n",
    "    self.distributions = []\n",
    "    for col in columns:\n",
    "      # must consider only not nans\n",
    "      notnans_mask = ~np.isnan(col)\n",
    "      # must create distribution for each column now\n",
    "      unique_vals, counts = np.unique(col[notnans_mask], return_counts=True)\n",
    "      probabilities = counts / counts.sum()\n",
    "      distribution = rv_discrete(values=(unique_vals, probabilities))\n",
    "      self.distributions.append(distribution)\n",
    "    \n",
    "    return self\n",
    "\n",
    "  def fit_transform(self, X, y = None):\n",
    "    # fit first to create distributions\n",
    "    self.fit(X, y)\n",
    "\n",
    "    # transform\n",
    "    return self.transform(X)\n",
    "    \n",
    "\n",
    "  def transform(self, X):\n",
    "    columns = []\n",
    "\n",
    "    # cast to float because only floats support np.nan\n",
    "    if isinstance(X, pd.DataFrame):\n",
    "      columns = X.values.T.copy().astype(np.float)\n",
    "    elif isinstance(X, np.ndarray):\n",
    "      columns = X.T.copy().astype(np.float)\n",
    "    else:\n",
    "      print(\"given X is neither DataFrame nor ndarray\")\n",
    "\n",
    "    for i, col in enumerate(columns):\n",
    "      distribution = self.distributions[i]\n",
    "      nan_cells_mask = np.isnan(col)\n",
    "      # careful here, where returns a one sized tuple (<value>, )\n",
    "      nan_cells_indices, = np.where(nan_cells_mask)\n",
    "      random_values = distribution.rvs(size=nan_cells_indices.size)\n",
    "      for index, random in zip(nan_cells_indices, random_values):\n",
    "        col[index] = random\n",
    "\n",
    "    return columns.T\n",
    "\n",
    "  def get_params(self, deep=True):\n",
    "    return {}\n",
    "\n",
    "  def set_params(self, **kwargs):\n",
    "    return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# нахожу ближайший ответ из массива уникальных значения для каждой фичи\n",
    "# переопределяю сырое значение после импутера на ближайшее к нему\n",
    "def dist(data, labels,idx):\n",
    "  for i in idx:\n",
    "    distance = []\n",
    "    for j in range(len(labels)):\n",
    "      distance.append(abs(data[i]-labels[j]))\n",
    "    min_dist = min(distance)\n",
    "    min_idx = distance.index(min_dist)\n",
    "    data[i] = labels[min_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "impute nan using KNN algorithm but slightly modified\n",
    "By default KNNImputer will calculate means (or medians), this implementation will instead look for closest \n",
    "values (for discrete features)\n",
    "'''\n",
    "from sklearn.impute import KNNImputer\n",
    "class NaNImputer(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, *discrete_columns_with_nans):\n",
    "        self.columns_with_nans = discrete_columns_with_nans\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        idx = np.array(X.index[X.isnull().any(axis=1)].tolist()) # индексы строк, в которых есть наны, чтобы прогонять цикл только по ним\n",
    "        g_lift_unique = X.g_lift.unique()  # убираю наны из списка уникальных значений\n",
    "        g_lift_unique = g_lift_unique[~np.isnan(g_lift_unique)]\n",
    "        build_tech_unique = X.build_tech.unique()\n",
    "        build_tech_unique = build_tech_unique[~np.isnan(build_tech_unique)]\n",
    "        metro_dist_unique = X.metro_dist.unique()\n",
    "        metro_dist_unique = metro_dist_unique[~np.isnan(metro_dist_unique)]\n",
    "        imputer = KNNImputer(n_neighbors=3, weights='uniform', metric='nan_euclidean') # число соседей подобрал эмпирически\n",
    "        filled_data = imputer.fit_transform(X.values)\n",
    "        # работает импутер не до конца корректно и на бинарных признаках может давать промежуточные значения\n",
    "        # подчищаю за импутером\n",
    "        filled_df = pd.DataFrame(filled_data,columns=X.columns)\n",
    "        dist(filled_df.g_lift,g_lift_unique,idx)\n",
    "        dist(filled_df.build_tech,build_tech_unique,idx)\n",
    "        dist(filled_df.metro_dist,metro_dist_unique,idx)\n",
    "        return filled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aNICnvW4LGuB"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "finds a min date per Series (of type pd.Timestamp)\n",
    "then subtracts it from each date\n",
    "the result per each column is a column with amount of days\n",
    "'''\n",
    "class DatePreprocessor():\n",
    "  def __init__(self, random_state=42):\n",
    "    self.min_dates = []\n",
    "\n",
    "  def fit(self, X, y=None):\n",
    "    # columns = []\n",
    "    # if (len(X.shape) == 1):\n",
    "    #   print(\"1d array is not supported, expected 2d\")\n",
    "\n",
    "    # if isinstance(X, pd.DataFrame):\n",
    "    #   columns = X.values.T\n",
    "    # elif isinstance(X, np.ndarray):\n",
    "    #   columns = X.T\n",
    "    # else:\n",
    "    #   print(\"given X is neither DataFrame nor ndarray\")\n",
    "\n",
    "    # for i in columns:\n",
    "    #   min_date, = X.min()\n",
    "    #   if isinstance(min_date, pd.Timestamp):\n",
    "    #     self.min_dates.append(min_date)\n",
    "    #   else:\n",
    "    #     print(type(min_date))\n",
    "    #     print(\"passed column is not of np.datetime64\")\n",
    "\n",
    "    return self\n",
    "\n",
    "  def fit_transform(self, X, y = None):\n",
    "    # fit first to create distributions\n",
    "    self.fit(X, y)\n",
    "\n",
    "    # transform\n",
    "    return self.transform(X)\n",
    "    \n",
    "\n",
    "  def transform(self, X):\n",
    "    columns = []\n",
    "    if (len(X.shape) == 1):\n",
    "      print(\"1d array is not supported, expected 2d\")\n",
    "\n",
    "    if isinstance(X, pd.DataFrame):\n",
    "      columns = X.values.T\n",
    "    elif isinstance(X, np.ndarray):\n",
    "      columns = X.T\n",
    "    else:\n",
    "      print(\"given X is neither DataFrame nor ndarray\")\n",
    "\n",
    "    print(columns.dtype)\n",
    "    dates = np.ones(X.T.shape, dtype=np.int)\n",
    "\n",
    "    for index, col in enumerate(columns):\n",
    "      dates[index] = np.array(list(map(lambda date: pd.Timestamp(date).month, col)))\n",
    "\n",
    "    return dates.T\n",
    "\n",
    "  def get_params(self, deep=True):\n",
    "    return {}\n",
    "\n",
    "  def set_params(self, **kwargs):\n",
    "    return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VhuApDK1MzDR"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "casts all passed columns to int\n",
    "\n",
    "may raise error if cannot cast series to int (if, e.g., nan is encountered)\n",
    "'''\n",
    "class CategoricalFeaturesToIntPreprocessor(BaseEstimator, TransformerMixin):\n",
    "  def __init__(self):\n",
    "      pass\n",
    "  \n",
    "  def fit(self, X, y=None):\n",
    "      return self\n",
    "  \n",
    "  def fit_transform(self, X, y = None):\n",
    "    # fit first to create distributions\n",
    "    self.fit(X, y)\n",
    "\n",
    "    # transform\n",
    "    return self.transform(X)\n",
    "  \n",
    "  def transform(self, X):\n",
    "    columns = []\n",
    "    print(type(X.head()))\n",
    "    print(X.head(), X.shape)\n",
    "    columns = X.T\n",
    "\n",
    "    for index in range(len(columns)):\n",
    "      columns[index] = columns[index].astype(np.int)\n",
    "    \n",
    "    return columns.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "drops nans\n",
    "'''\n",
    "class NanDropper(BaseEstimator, TransformerMixin):\n",
    "  def __init__(self):\n",
    "      pass\n",
    "  \n",
    "  def fit(self, X, y=None):\n",
    "      return self\n",
    "  \n",
    "  def fit_transform(self, X, y = None):\n",
    "    # fit first to create distributions\n",
    "    self.fit(X, y)\n",
    "\n",
    "    # transform\n",
    "    return self.transform(X)\n",
    "  \n",
    "  def transform(self, X, *y):\n",
    "    X = X.dropna()\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pandarizer(columns, *placed_to_first, **options):\n",
    "    if (len(placed_to_first) != 0):\n",
    "        columns = names_at_start(columns, *placed_to_first)\n",
    "        \n",
    "    def framerize(x):\n",
    "        frame = pd.DataFrame(x, columns = columns)\n",
    "        if len(options) > 0:\n",
    "            int_columns = options['toint']\n",
    "            for int_column in int_columns:\n",
    "                frame[int_column] = frame[int_column].astype(np.int)\n",
    "        return frame\n",
    "            \n",
    "    return FunctionTransformer(lambda x: framerize(x))\n",
    "\n",
    "def names_at_start(columns, *placed_to_first):\n",
    "    columns = columns.copy()\n",
    "    if not isinstance(columns, list):\n",
    "        columns = columns.tolist()\n",
    "        \n",
    "    for name in placed_to_first[::-1]:\n",
    "        if name not in columns:\n",
    "            print(name, \"not in list\")\n",
    "        columns.remove(name)\n",
    "        columns.insert(0, name)\n",
    "    return columns\n",
    "\n",
    "def without(columns, *without_names):\n",
    "    columns = columns.copy()\n",
    "    if not isinstance(columns, list):\n",
    "        columns = columns.tolist()\n",
    "        \n",
    "    for name in without_names:\n",
    "        columns.remove(name)\n",
    "    return columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "llv4CUjXtoWi"
   },
   "source": [
    "## Подгрузить данные\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FqmALDpcpCPg"
   },
   "outputs": [],
   "source": [
    "gdrive_path_train = \"/content/drive/MyDrive/Технопарк 2021 ML/lecture05/Train.csv\"\n",
    "local_path_train = \"Train.csv\"\n",
    "gdrive_path_test = \"/content/drive/MyDrive/Технопарк 2021 ML/lecture05/Test.csv\"\n",
    "local_path_test = \"Test.csv\"\n",
    "\n",
    "data_train = pd.read_csv(local_path_train, index_col=None)\n",
    "data_test = pd.read_csv(local_path_test, index_col=None)\n",
    "\n",
    "data_train['date'] = pd.to_datetime(data_train['date'])\n",
    "data_test['date'] = pd.to_datetime(data_test['date'])\n",
    "data_train.drop(data_train.columns[[0]], axis=1,inplace=True)\n",
    "data_test.drop(data_test.columns[[0]], axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 394
    },
    "id": "vjmBIeYsgNb-",
    "outputId": "be481506-6b34-4f63-c74a-6b4a79406a5b"
   },
   "outputs": [],
   "source": [
    "data_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xr2SIE22vfFl"
   },
   "source": [
    "## Поиск модели для предсказания стоимости квартиры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q0IRxLR1Dfd4"
   },
   "outputs": [],
   "source": [
    "X_train_train, X_train_test, y_train_train, y_train_test = train_test_split(data_train.iloc[:, :-1], data_train.iloc[:, -1], shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tO4RXC9oMXVx"
   },
   "source": [
    "Относительная погрешность\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5Z7JibdpMZ5l"
   },
   "outputs": [],
   "source": [
    "def relative_error(y_true, y_pred): \n",
    "  return 1 - (np.abs((y_true - y_pred) / y_train_test).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cDG4ay_rOePi"
   },
   "source": [
    "### CatBoost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nsmaj2J1OTQm"
   },
   "outputs": [],
   "source": [
    "from catboost import Pool, CatBoostRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_model = make_pipeline(\n",
    "    make_column_transformer(\n",
    "        # convert date to months\n",
    "        (DatePreprocessor(), ['date']),\n",
    "        # fill nans for columns using their distributions\n",
    "        (DiscreteDistributionImputer(), ['build_tech', 'g_lift']),\n",
    "        # fill left nans simply\n",
    "        remainder=SimpleImputer()\n",
    "    ),\n",
    "    # pandarize for other columntransform conversion, date, build_Tech and g_lift labels go first after first CT conversion\n",
    "    pandarizer(X_labels, 'date', 'build_tech', 'g_lift'),\n",
    "    # drop kw columns that are highly disbalanced\n",
    "#     make_column_transformer(\n",
    "#         ('drop', [\"kw{}\".format(i) for i in range(1, 14) if i != 2]),\n",
    "#         remainder='passthrough'\n",
    "#     ),\n",
    "    # pandarize again to check that everything is as must\n",
    "#     pandarizer(without(X_labels, *[\"kw{}\".format(i) for i in range(1, 14) if i != 2]), \n",
    "#                'date', 'build_tech', 'g_lift',\n",
    "#                toint=['date', 'build_tech', 'g_lift', 'street_id', 'floor', 'area', 'rooms', 'balcon', 'n_photos', 'kw2']\n",
    "#               ),\n",
    "    pandarizer(X_labels, \n",
    "           'date', 'build_tech', 'g_lift',\n",
    "           toint=['date', 'build_tech', 'g_lift', 'street_id', 'floor', 'area', 'rooms', 'balcon', 'n_photos', 'kw2']\n",
    "          ),\n",
    "    'passthrough'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_model.fit_transform(X_train_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h8ulzvgsVCGK"
   },
   "outputs": [],
   "source": [
    "model = CatBoostRegressor(learning_rate=0.5, depth=10, eval_metric = 'MAE', task_type='GPU', iterations=3000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nuCFr2o1VSH6"
   },
   "outputs": [],
   "source": [
    "pipeline_model = make_pipeline(\n",
    "    preprocess_model,\n",
    "    model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_model.fit(X_train_train, y_train_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KcIx0AEUViJW",
    "outputId": "e2d3e26e-f1d0-4775-dc5f-b7bcf076ca56"
   },
   "outputs": [],
   "source": [
    "y_train_test_pred = pipeline_model.predict(X_train_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OY0-0WumVuoI",
    "outputId": "db037037-489b-4f61-f1ab-e957cdb389c4"
   },
   "outputs": [],
   "source": [
    "relative_error(y_train_test, y_train_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rLUuSUfNr8vY",
    "outputId": "3e04b33a-05af-4464-e784-d4fe1f2d4d52"
   },
   "outputs": [],
   "source": [
    "mean_absolute_error(y_train_test,y_train_test_pred )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zHn4mfR5Vv7t"
   },
   "source": [
    "Пока качество не айс. Ну чтош, сравню хотя бы как эта штука отрабатывает, и SimpleSolution в кегле на тестовых данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_model.fit_transform(X_train_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_model = make_pipeline(\n",
    "    make_column_transformer(\n",
    "        # convert date to months\n",
    "        (DatePreprocessor(), ['date']),\n",
    "        # fill nans for columns using their distributions\n",
    "#         (DiscreteDistributionImputer(), ['build_tech', 'g_lift']),\n",
    "        # fill left nans simply\n",
    "        remainder='passthrough'\n",
    "    ),\n",
    "    # pandarize for other columntransform conversion, date, build_Tech and g_lift labels go first after first CT conversion\n",
    "    pandarizer(X_train_train.columns, 'date'),\n",
    "    # drop nans\n",
    "#     NanDropper(),\n",
    "    # drop kw columns that are highly disbalanced\n",
    "#     make_column_transformer(\n",
    "#         ('drop', [\"kw{}\".format(i) for i in range(1, 14) if i != 2]),\n",
    "#         remainder='passthrough'\n",
    "#     ),\n",
    "    # pandarize again to check that everything is as must\n",
    "#     pandarizer(without(X_labels, *[\"kw{}\".format(i) for i in range(1, 14) if i != 2]), \n",
    "#                'date', 'build_tech', 'g_lift',\n",
    "#                toint=['date', 'build_tech', 'g_lift', 'street_id', 'floor', 'area', 'rooms', 'balcon', 'n_photos', 'kw2']\n",
    "#               ),\n",
    "#     pandarizer(X_train_train.columns, \n",
    "#            'date',\n",
    "#            toint=['date', 'build_tech', 'g_lift', 'street_id', 'floor', 'area', 'rooms', 'balcon', 'n_photos', 'kw2']\n",
    "#           ),\n",
    "    'passthrough'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_model_normal = make_pipeline(\n",
    "    make_column_transformer(\n",
    "        # convert date to months\n",
    "        (DatePreprocessor(), ['date']),\n",
    "        # fill nans for columns using their distributions\n",
    "        (DiscreteDistributionImputer(), ['build_tech', 'g_lift']),\n",
    "        # fill left nans simply\n",
    "        remainder=SimpleImputer()\n",
    "    ),\n",
    "    # pandarize for other columntransform conversion, date, build_Tech and g_lift labels go first after first CT conversion\n",
    "    pandarizer(X_train_train.columns, 'date', 'build_tech', 'g_lift'),\n",
    "    # drop kw columns that are highly disbalanced\n",
    "#     make_column_transformer(\n",
    "#         ('drop', [\"kw{}\".format(i) for i in range(1, 14) if i != 2]),\n",
    "#         remainder='passthrough'\n",
    "#     ),\n",
    "    # pandarize again to check that everything is as must\n",
    "#     pandarizer(without(X_labels, *[\"kw{}\".format(i) for i in range(1, 14) if i != 2]), \n",
    "#                'date', 'build_tech', 'g_lift',\n",
    "#                toint=['date', 'build_tech', 'g_lift', 'street_id', 'floor', 'area', 'rooms', 'balcon', 'n_photos', 'kw2']\n",
    "#               ),\n",
    "#     pandarizer(X_train_train.columns, \n",
    "#            'date',\n",
    "#            toint=['date', 'build_tech', 'g_lift', 'street_id', 'floor', 'area', 'rooms', 'balcon', 'n_photos', 'kw2']\n",
    "#           ),\n",
    "    'passthrough'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_prep = preprocess_model.fit_transform(X_train_train).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_prep = pd.DataFrame(np.hstack([data_train_prep, y_train_train[:, np.newaxis]]))\n",
    "data_train_prep = data_train_prep.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_model = LGBMRegressor()\n",
    "lgbm_model.fit(data_train_prep.iloc[:, :-1], data_train_prep.iloc[:, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicts = lgbm_model.predict(preprocess_model.fit_transform(X_train_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_error(y_train_test, predicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_absolute_error(y_train_test, predicts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n4dHaVA7dPUV"
   },
   "source": [
    "## Использование выбранной модели на тесте"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9NB3_O9-gJps"
   },
   "outputs": [],
   "source": [
    "y_test_pred = pipeline.predict(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xL7yPS5-hQEO"
   },
   "outputs": [],
   "source": [
    "answers = np.hstack((np.arange(100000, 200000)[:, np.newaxis], y_test_pred[:, np.newaxis]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 450
    },
    "id": "p52u_dGahnwW",
    "outputId": "00d0a0c1-a6cc-4e3c-c52a-ba1556877b3c"
   },
   "outputs": [],
   "source": [
    "answers_pd = pd.DataFrame(answers, columns=[\"id\", \"price\"])\n",
    "answers_pd['id'] = answers_pd['id'].astype(np.int)\n",
    "answers_pd.set_index('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cUMLGbDpijqt"
   },
   "outputs": [],
   "source": [
    "answers_pd.to_csv(\"/content/drive/MyDrive/Технопарк 2021 ML/lecture05/submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cAIyhosyikaf"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N-vCTap6BXlk"
   },
   "outputs": [],
   "source": [
    "data_train = pd.read_csv(\"/content/drive/MyDrive/Технопарк 2021 ML/lecture05/Train.csv\", index_col=None)\n",
    "data_test = pd.read_csv(\"/content/drive/MyDrive/Технопарк 2021 ML/lecture05/Test.csv\", index_col=None)\n",
    "\n",
    "data_train['date'] = pd.to_datetime(data_train['date'])\n",
    "data_test['date'] = pd.to_datetime(data_test['date'])\n",
    "data_train.drop(data_train.columns[[0]], axis=1,inplace=True)\n",
    "data_test.drop(data_test.columns[[0]], axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3T6sf82aDGtW"
   },
   "outputs": [],
   "source": [
    "model = CatBoostRegressor(iterations=1000,\n",
    "                          l2_leaf_reg= 1,\n",
    "                          depth=6,\n",
    "                          eval_metric='MAE',\n",
    "                           learning_rate=0.6,\n",
    "                           loss_function='RMSE',\n",
    "                           verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vp4v-vEGBe6l"
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline(steps=[\n",
    "                           ('preproc', ColumnTransformer([\n",
    "                                              ('drop', 'drop', ['date']),\n",
    "                                              ('impute', SimpleImputer(strategy='median'), ['metro_dist', 'build_tech', 'g_lift'])\n",
    "                           ], remainder='passthrough')),\n",
    "                           ('model', model)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CaHp7KDUCeRx",
    "outputId": "e02a0020-b2d5-43ec-85dc-ab1dad9cbec1"
   },
   "outputs": [],
   "source": [
    "pipeline.fit(data_train.iloc[:, :-1], data_train.iloc[:, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0iLIjZqmDeVk"
   },
   "outputs": [],
   "source": [
    "predicts = pipeline.predict(X_train_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sEb7nSzxD--7",
    "outputId": "7fc77652-480c-45f7-d791-b88ce27505e0"
   },
   "outputs": [],
   "source": [
    "relative_error(predicts, data_train.iloc[:, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YEWKMfS5EOCm"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "ebEXDRPKMfTp"
   ],
   "name": "dz3_kaggle.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
